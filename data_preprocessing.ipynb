{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab513aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import joblib\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from imblearn.over_sampling import SMOTE   \n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "176ba061",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./mtsamples.csv') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd56f0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentences (limited to 20): ['SUBJECTIVE:,  This 23-year-old white female presents with complaint of allergies.', 'She used to have allergies when she lived in Seattle but she thinks they are worse here.', 'In the past, she has tried Claritin, and Zyrtec.', 'Both worked for short time but then seemed to lose effectiveness.', 'She has used Allegra also.', 'She used that last summer and she began using it again two weeks ago.', 'It does not appear to be working very well.', 'She has used over-the-counter sprays but no prescription nasal sprays.', 'She does have asthma but doest not require daily medication for this and does not think it is flaring up.,MEDICATIONS: , Her only medication currently is Ortho Tri-Cyclen and the Allegra.,ALLERGIES: , She has no known medicine allergies.,OBJECTIVE:,Vitals:  Weight was 130 pounds and blood pressure 124/78.,HEENT:  Her throat was mildly erythematous without exudate.', 'Nasal mucosa was erythematous and swollen.', 'Only clear drainage was seen.', 'TMs were clear.,Neck:  Supple without adenopathy.,Lungs:  Clear.,ASSESSMENT:,  Allergic rhinitis.,PLAN:,1.', 'She will try Zyrtec instead of Allegra again.', 'Another option will be to use loratadine.', 'She does not think she has prescription coverage so that might be cheaper.,2.', 'Samples of Nasonex two sprays in each nostril given for three weeks.', 'A prescription was written as well.', 'PAST MEDICAL HISTORY:, He has difficulty climbing stairs, difficulty with airline seats, tying shoes, used to public seating, and lifting objects off the floor.', 'He exercises three times a week at home and does cardio.', 'He has difficulty walking two blocks or five flights of stairs.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "data['transcription'] = data['transcription'].astype(str)\n",
    "\n",
    "# Extract the 'transcription' column from the DataFrame\n",
    "sentences = data['transcription'].tolist()\n",
    "\n",
    "# Print the sentences before joining\n",
    "print(f\"Sentences before joining: {sentences}\")\n",
    "\n",
    "# Concatenate all sentences into a single string\n",
    "all_text = ' '.join(sentences)\n",
    "\n",
    "# Print the concatenated text\n",
    "print(f\"All text: {all_text}\")\n",
    "\n",
    "# Tokenize the entire text\n",
    "tokenized_sentences = nltk.sent_tokenize(all_text)\n",
    "\n",
    "# Limit the output to the first 20 tokenized sentences\n",
    "tokenized_sentences = tokenized_sentences[:20]\n",
    "\n",
    "# Print the tokenized sentences\n",
    "print(f\"Tokenized Sentences (limited to 20): {tokenized_sentences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e8f7d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3a45188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                        description  \\\n",
      "0           0   A 23-year-old white female presents with comp...   \n",
      "1           1           Consult for laparoscopic gastric bypass.   \n",
      "2           2           Consult for laparoscopic gastric bypass.   \n",
      "3           3                             2-D M-Mode. Doppler.     \n",
      "4           4                                 2-D Echocardiogram   \n",
      "\n",
      "             medical_specialty                                sample_name  \\\n",
      "0         Allergy / Immunology                         Allergic Rhinitis    \n",
      "1                   Bariatrics   Laparoscopic Gastric Bypass Consult - 2    \n",
      "2                   Bariatrics   Laparoscopic Gastric Bypass Consult - 1    \n",
      "3   Cardiovascular / Pulmonary                    2-D Echocardiogram - 1    \n",
      "4   Cardiovascular / Pulmonary                    2-D Echocardiogram - 2    \n",
      "\n",
      "                                       transcription  \\\n",
      "0  SUBJECTIVE:,  This 23-year-old white female pr...   \n",
      "1  PAST MEDICAL HISTORY:, He has difficulty climb...   \n",
      "2  HISTORY OF PRESENT ILLNESS: , I have seen ABC ...   \n",
      "3  2-D M-MODE: , ,1.  Left atrial enlargement wit...   \n",
      "4  1.  The left ventricular cavity size and wall ...   \n",
      "\n",
      "                                            keywords  \n",
      "0  allergy / immunology, allergic rhinitis, aller...  \n",
      "1  bariatrics, laparoscopic gastric bypass, weigh...  \n",
      "2  bariatrics, laparoscopic gastric bypass, heart...  \n",
      "3  cardiovascular / pulmonary, 2-d m-mode, dopple...  \n",
      "4  cardiovascular / pulmonary, 2-d, doppler, echo...  \n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d042d625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (3.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (4.47.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "357dcf03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in Sentence 1: ['SUBJECTIVE', ':', ',', 'This', '23-year-old', 'white', 'female', 'presents', 'with', 'complaint', 'of', 'allergies', '.']\n",
      "Words in Sentence 2: ['She', 'used', 'to', 'have', 'allergies', 'when', 'she', 'lived', 'in', 'Seattle', 'but', 'she', 'thinks', 'they', 'are', 'worse', 'here', '.']\n",
      "Words in Sentence 3: ['In', 'the', 'past', ',', 'she', 'has', 'tried', 'Claritin', ',', 'and', 'Zyrtec', '.']\n",
      "Words in Sentence 4: ['Both', 'worked', 'for', 'short', 'time', 'but', 'then', 'seemed', 'to', 'lose', 'effectiveness', '.']\n",
      "Words in Sentence 5: ['She', 'has', 'used', 'Allegra', 'also', '.']\n",
      "Words in Sentence 6: ['She', 'used', 'that', 'last', 'summer', 'and', 'she', 'began', 'using', 'it', 'again', 'two', 'weeks', 'ago', '.']\n",
      "Words in Sentence 7: ['It', 'does', 'not', 'appear', 'to', 'be', 'working', 'very', 'well', '.']\n",
      "Words in Sentence 8: ['She', 'has', 'used', 'over-the-counter', 'sprays', 'but', 'no', 'prescription', 'nasal', 'sprays', '.']\n",
      "Words in Sentence 9: ['She', 'does', 'have', 'asthma', 'but', 'doest', 'not', 'require', 'daily', 'medication', 'for', 'this', 'and', 'does', 'not', 'think', 'it', 'is', 'flaring', 'up.', ',', 'MEDICATIONS', ':', ',', 'Her', 'only', 'medication', 'currently', 'is', 'Ortho', 'Tri-Cyclen', 'and', 'the', 'Allegra.', ',', 'ALLERGIES', ':', ',', 'She', 'has', 'no', 'known', 'medicine', 'allergies.', ',', 'OBJECTIVE', ':', ',Vitals', ':', 'Weight', 'was', '130', 'pounds', 'and', 'blood', 'pressure', '124/78.', ',', 'HEENT', ':', 'Her', 'throat', 'was', 'mildly', 'erythematous', 'without', 'exudate', '.']\n",
      "Words in Sentence 10: ['Nasal', 'mucosa', 'was', 'erythematous', 'and', 'swollen', '.']\n",
      "Words in Sentence 11: ['Only', 'clear', 'drainage', 'was', 'seen', '.']\n",
      "Words in Sentence 12: ['TMs', 'were', 'clear.', ',', 'Neck', ':', 'Supple', 'without', 'adenopathy.', ',', 'Lungs', ':', 'Clear.', ',', 'ASSESSMENT', ':', ',', 'Allergic', 'rhinitis.', ',', 'PLAN', ':', ',1', '.']\n",
      "Words in Sentence 13: ['She', 'will', 'try', 'Zyrtec', 'instead', 'of', 'Allegra', 'again', '.']\n",
      "Words in Sentence 14: ['Another', 'option', 'will', 'be', 'to', 'use', 'loratadine', '.']\n",
      "Words in Sentence 15: ['She', 'does', 'not', 'think', 'she', 'has', 'prescription', 'coverage', 'so', 'that', 'might', 'be', 'cheaper.,2', '.']\n",
      "Words in Sentence 16: ['Samples', 'of', 'Nasonex', 'two', 'sprays', 'in', 'each', 'nostril', 'given', 'for', 'three', 'weeks', '.']\n",
      "Words in Sentence 17: ['A', 'prescription', 'was', 'written', 'as', 'well', '.']\n",
      "Words in Sentence 18: ['PAST', 'MEDICAL', 'HISTORY', ':', ',', 'He', 'has', 'difficulty', 'climbing', 'stairs', ',', 'difficulty', 'with', 'airline', 'seats', ',', 'tying', 'shoes', ',', 'used', 'to', 'public', 'seating', ',', 'and', 'lifting', 'objects', 'off', 'the', 'floor', '.']\n",
      "Words in Sentence 19: ['He', 'exercises', 'three', 'times', 'a', 'week', 'at', 'home', 'and', 'does', 'cardio', '.']\n",
      "Words in Sentence 20: ['He', 'has', 'difficulty', 'walking', 'two', 'blocks', 'or', 'five', 'flights', 'of', 'stairs', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize each sentence into words\n",
    "tokenized_words = [nltk.word_tokenize(sentence) for sentence in tokenized_sentences]\n",
    "\n",
    "# Display the tokenized words for each sentence\n",
    "for i, words in enumerate(tokenized_words):\n",
    "    print(f\"Words in Sentence {i + 1}: {words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3e55e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Words in Sentence 1: ['subject', ':', ',', 'thi', '23-year-old', 'white', 'femal', 'present', 'with', 'complaint', 'of', 'allergi', '.']\n",
      "Stemmed Words in Sentence 2: ['she', 'use', 'to', 'have', 'allergi', 'when', 'she', 'live', 'in', 'seattl', 'but', 'she', 'think', 'they', 'are', 'wors', 'here', '.']\n",
      "Stemmed Words in Sentence 3: ['in', 'the', 'past', ',', 'she', 'ha', 'tri', 'claritin', ',', 'and', 'zyrtec', '.']\n",
      "Stemmed Words in Sentence 4: ['both', 'work', 'for', 'short', 'time', 'but', 'then', 'seem', 'to', 'lose', 'effect', '.']\n",
      "Stemmed Words in Sentence 5: ['she', 'ha', 'use', 'allegra', 'also', '.']\n",
      "Stemmed Words in Sentence 6: ['she', 'use', 'that', 'last', 'summer', 'and', 'she', 'began', 'use', 'it', 'again', 'two', 'week', 'ago', '.']\n",
      "Stemmed Words in Sentence 7: ['it', 'doe', 'not', 'appear', 'to', 'be', 'work', 'veri', 'well', '.']\n",
      "Stemmed Words in Sentence 8: ['she', 'ha', 'use', 'over-the-count', 'spray', 'but', 'no', 'prescript', 'nasal', 'spray', '.']\n",
      "Stemmed Words in Sentence 9: ['she', 'doe', 'have', 'asthma', 'but', 'doest', 'not', 'requir', 'daili', 'medic', 'for', 'thi', 'and', 'doe', 'not', 'think', 'it', 'is', 'flare', 'up.', ',', 'medic', ':', ',', 'her', 'onli', 'medic', 'current', 'is', 'ortho', 'tri-cyclen', 'and', 'the', 'allegra.', ',', 'allergi', ':', ',', 'she', 'ha', 'no', 'known', 'medicin', 'allergies.', ',', 'object', ':', ',vital', ':', 'weight', 'wa', '130', 'pound', 'and', 'blood', 'pressur', '124/78.', ',', 'heent', ':', 'her', 'throat', 'wa', 'mildli', 'erythemat', 'without', 'exud', '.']\n",
      "Stemmed Words in Sentence 10: ['nasal', 'mucosa', 'wa', 'erythemat', 'and', 'swollen', '.']\n",
      "Stemmed Words in Sentence 11: ['onli', 'clear', 'drainag', 'wa', 'seen', '.']\n",
      "Stemmed Words in Sentence 12: ['tm', 'were', 'clear.', ',', 'neck', ':', 'suppl', 'without', 'adenopathy.', ',', 'lung', ':', 'clear.', ',', 'assess', ':', ',', 'allerg', 'rhinitis.', ',', 'plan', ':', ',1', '.']\n",
      "Stemmed Words in Sentence 13: ['she', 'will', 'tri', 'zyrtec', 'instead', 'of', 'allegra', 'again', '.']\n",
      "Stemmed Words in Sentence 14: ['anoth', 'option', 'will', 'be', 'to', 'use', 'loratadin', '.']\n",
      "Stemmed Words in Sentence 15: ['she', 'doe', 'not', 'think', 'she', 'ha', 'prescript', 'coverag', 'so', 'that', 'might', 'be', 'cheaper.,2', '.']\n",
      "Stemmed Words in Sentence 16: ['sampl', 'of', 'nasonex', 'two', 'spray', 'in', 'each', 'nostril', 'given', 'for', 'three', 'week', '.']\n",
      "Stemmed Words in Sentence 17: ['a', 'prescript', 'wa', 'written', 'as', 'well', '.']\n",
      "Stemmed Words in Sentence 18: ['past', 'medic', 'histori', ':', ',', 'he', 'ha', 'difficulti', 'climb', 'stair', ',', 'difficulti', 'with', 'airlin', 'seat', ',', 'tie', 'shoe', ',', 'use', 'to', 'public', 'seat', ',', 'and', 'lift', 'object', 'off', 'the', 'floor', '.']\n",
      "Stemmed Words in Sentence 19: ['he', 'exercis', 'three', 'time', 'a', 'week', 'at', 'home', 'and', 'doe', 'cardio', '.']\n",
      "Stemmed Words in Sentence 20: ['he', 'ha', 'difficulti', 'walk', 'two', 'block', 'or', 'five', 'flight', 'of', 'stair', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "# Create a PorterStemmer instance\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# Apply stemming to each word in each sentence\n",
    "stemmed_words = [[porter_stemmer.stem(word) for word in words] for words in tokenized_words]\n",
    "\n",
    "# Display the stemmed words for each sentence\n",
    "for i, words in enumerate(stemmed_words):\n",
    "    print(f\"Stemmed Words in Sentence {i + 1}: {words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66971d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words in Sentence 1: ['SUBJECTIVE', ':', ',', 'This', '23-year-old', 'white', 'female', 'present', 'with', 'complaint', 'of', 'allergy', '.']\n",
      "Lemmatized Words in Sentence 2: ['She', 'used', 'to', 'have', 'allergy', 'when', 'she', 'lived', 'in', 'Seattle', 'but', 'she', 'think', 'they', 'are', 'worse', 'here', '.']\n",
      "Lemmatized Words in Sentence 3: ['In', 'the', 'past', ',', 'she', 'ha', 'tried', 'Claritin', ',', 'and', 'Zyrtec', '.']\n",
      "Lemmatized Words in Sentence 4: ['Both', 'worked', 'for', 'short', 'time', 'but', 'then', 'seemed', 'to', 'lose', 'effectiveness', '.']\n",
      "Lemmatized Words in Sentence 5: ['She', 'ha', 'used', 'Allegra', 'also', '.']\n",
      "Lemmatized Words in Sentence 6: ['She', 'used', 'that', 'last', 'summer', 'and', 'she', 'began', 'using', 'it', 'again', 'two', 'week', 'ago', '.']\n",
      "Lemmatized Words in Sentence 7: ['It', 'doe', 'not', 'appear', 'to', 'be', 'working', 'very', 'well', '.']\n",
      "Lemmatized Words in Sentence 8: ['She', 'ha', 'used', 'over-the-counter', 'spray', 'but', 'no', 'prescription', 'nasal', 'spray', '.']\n",
      "Lemmatized Words in Sentence 9: ['She', 'doe', 'have', 'asthma', 'but', 'doest', 'not', 'require', 'daily', 'medication', 'for', 'this', 'and', 'doe', 'not', 'think', 'it', 'is', 'flaring', 'up.', ',', 'MEDICATIONS', ':', ',', 'Her', 'only', 'medication', 'currently', 'is', 'Ortho', 'Tri-Cyclen', 'and', 'the', 'Allegra.', ',', 'ALLERGIES', ':', ',', 'She', 'ha', 'no', 'known', 'medicine', 'allergies.', ',', 'OBJECTIVE', ':', ',Vitals', ':', 'Weight', 'wa', '130', 'pound', 'and', 'blood', 'pressure', '124/78.', ',', 'HEENT', ':', 'Her', 'throat', 'wa', 'mildly', 'erythematous', 'without', 'exudate', '.']\n",
      "Lemmatized Words in Sentence 10: ['Nasal', 'mucosa', 'wa', 'erythematous', 'and', 'swollen', '.']\n",
      "Lemmatized Words in Sentence 11: ['Only', 'clear', 'drainage', 'wa', 'seen', '.']\n",
      "Lemmatized Words in Sentence 12: ['TMs', 'were', 'clear.', ',', 'Neck', ':', 'Supple', 'without', 'adenopathy.', ',', 'Lungs', ':', 'Clear.', ',', 'ASSESSMENT', ':', ',', 'Allergic', 'rhinitis.', ',', 'PLAN', ':', ',1', '.']\n",
      "Lemmatized Words in Sentence 13: ['She', 'will', 'try', 'Zyrtec', 'instead', 'of', 'Allegra', 'again', '.']\n",
      "Lemmatized Words in Sentence 14: ['Another', 'option', 'will', 'be', 'to', 'use', 'loratadine', '.']\n",
      "Lemmatized Words in Sentence 15: ['She', 'doe', 'not', 'think', 'she', 'ha', 'prescription', 'coverage', 'so', 'that', 'might', 'be', 'cheaper.,2', '.']\n",
      "Lemmatized Words in Sentence 16: ['Samples', 'of', 'Nasonex', 'two', 'spray', 'in', 'each', 'nostril', 'given', 'for', 'three', 'week', '.']\n",
      "Lemmatized Words in Sentence 17: ['A', 'prescription', 'wa', 'written', 'a', 'well', '.']\n",
      "Lemmatized Words in Sentence 18: ['PAST', 'MEDICAL', 'HISTORY', ':', ',', 'He', 'ha', 'difficulty', 'climbing', 'stair', ',', 'difficulty', 'with', 'airline', 'seat', ',', 'tying', 'shoe', ',', 'used', 'to', 'public', 'seating', ',', 'and', 'lifting', 'object', 'off', 'the', 'floor', '.']\n",
      "Lemmatized Words in Sentence 19: ['He', 'exercise', 'three', 'time', 'a', 'week', 'at', 'home', 'and', 'doe', 'cardio', '.']\n",
      "Lemmatized Words in Sentence 20: ['He', 'ha', 'difficulty', 'walking', 'two', 'block', 'or', 'five', 'flight', 'of', 'stair', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "# Create a WordNetLemmatizer instance\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply lemmatization to each word in each sentence\n",
    "lemmatized_words = [[lemmatizer.lemmatize(word) for word in words] for words in tokenized_words]\n",
    "\n",
    "# Display the lemmatized words for each sentence\n",
    "for i, words in enumerate(lemmatized_words):\n",
    "    print(f\"Lemmatized Words in Sentence {i + 1}: {words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77029311",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\priya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\priya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "   Unnamed: 0                                        description  \\\n",
      "0           0   A 23-year-old white female presents with comp...   \n",
      "1           1           Consult for laparoscopic gastric bypass.   \n",
      "2           2           Consult for laparoscopic gastric bypass.   \n",
      "3           3                             2-D M-Mode. Doppler.     \n",
      "4           4                                 2-D Echocardiogram   \n",
      "\n",
      "             medical_specialty                                sample_name  \\\n",
      "0         Allergy / Immunology                         Allergic Rhinitis    \n",
      "1                   Bariatrics   Laparoscopic Gastric Bypass Consult - 2    \n",
      "2                   Bariatrics   Laparoscopic Gastric Bypass Consult - 1    \n",
      "3   Cardiovascular / Pulmonary                    2-D Echocardiogram - 1    \n",
      "4   Cardiovascular / Pulmonary                    2-D Echocardiogram - 2    \n",
      "\n",
      "                                       transcription  \\\n",
      "0  SUBJECTIVE:,  This 23-year-old white female pr...   \n",
      "1  PAST MEDICAL HISTORY:, He has difficulty climb...   \n",
      "2  HISTORY OF PRESENT ILLNESS: , I have seen ABC ...   \n",
      "3  2-D M-MODE: , ,1.  Left atrial enlargement wit...   \n",
      "4  1.  The left ventricular cavity size and wall ...   \n",
      "\n",
      "                                            keywords  \n",
      "0  allergy / immunology, allergic rhinitis, aller...  \n",
      "1  bariatrics, laparoscopic gastric bypass, weigh...  \n",
      "2  bariatrics, laparoscopic gastric bypass, heart...  \n",
      "3  cardiovascular / pulmonary, 2-d m-mode, dopple...  \n",
      "4  cardiovascular / pulmonary, 2-d, doppler, echo...  \n",
      "\n",
      "DataFrame with Cleaned Text:\n",
      "                                          transcription  \\\n",
      "0     SUBJECTIVE:,  This 23-year-old white female pr...   \n",
      "1     PAST MEDICAL HISTORY:, He has difficulty climb...   \n",
      "2     HISTORY OF PRESENT ILLNESS: , I have seen ABC ...   \n",
      "3     2-D M-MODE: , ,1.  Left atrial enlargement wit...   \n",
      "4     1.  The left ventricular cavity size and wall ...   \n",
      "...                                                 ...   \n",
      "4994  HISTORY:,  I had the pleasure of meeting and e...   \n",
      "4995  ADMITTING DIAGNOSIS: , Kawasaki disease.,DISCH...   \n",
      "4996  SUBJECTIVE: , This is a 42-year-old white fema...   \n",
      "4997  CHIEF COMPLAINT: , This 5-year-old male presen...   \n",
      "4998  HISTORY: , A 34-year-old male presents today s...   \n",
      "\n",
      "                                  cleaned_transcription  \n",
      "0     subject 23yearold white femal present complain...  \n",
      "1     past medic histori difficulti climb stair diff...  \n",
      "2     histori present ill seen abc today pleasant ge...  \n",
      "3     2d mmode 1 left atrial enlarg left atrial diam...  \n",
      "4     1 left ventricular caviti size wall thick appe...  \n",
      "...                                                 ...  \n",
      "4994  histori pleasur meet evalu patient refer today...  \n",
      "4995  admit diagnosi kawasaki diseasedischarg diagno...  \n",
      "4996  subject 42yearold white femal come today compl...  \n",
      "4997  chief complaint 5yearold male present children...  \n",
      "4998  histori 34yearold male present today selfref r...  \n",
      "\n",
      "[4999 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def clean_text(text):\n",
    "    # Check if the text is not NaN\n",
    "    if isinstance(text, str):\n",
    "        # get English stopwords\n",
    "        english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "        # change to lower case and remove punctuation\n",
    "        text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        # divide string into individual words\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        stemmer = PorterStemmer()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        clean_tokens = []\n",
    "        for tok in tokens:\n",
    "            tok = tok.strip()  # remove space\n",
    "            if tok not in english_stopwords:\n",
    "                clean_tok = lemmatizer.lemmatize(tok)  # lemmatization\n",
    "                clean_tok = stemmer.stem(clean_tok)  # Stemming\n",
    "                clean_tokens.append(clean_tok)\n",
    "        return \" \".join(clean_tokens)\n",
    "    else:\n",
    "        return \"\"  # Return an empty string for NaN values\n",
    "\n",
    "# Read the dataset\n",
    "data = pd.read_csv('./mtsamples.csv')\n",
    "\n",
    "# Display the first few rows of the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "print(data.head())\n",
    "\n",
    "# Assuming the column you want to clean is 'transcription'\n",
    "data['cleaned_transcription'] = data['transcription'].apply(clean_text)\n",
    "\n",
    "# Display both the original and cleaned text columns\n",
    "print(\"\\nDataFrame with Cleaned Text:\")\n",
    "print(data[['transcription', 'cleaned_transcription']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41abf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def clean_text(text):\n",
    "    # Check if the text is not NaN\n",
    "    if isinstance(text, str):\n",
    "        # get English stopwords\n",
    "        english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "        # change to lower case and remove punctuation\n",
    "        text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        # divide string into individual words\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        stemmer = PorterStemmer()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        clean_tokens = []\n",
    "        for tok in tokens:\n",
    "            tok = tok.strip()  # remove space\n",
    "            if tok not in english_stopwords:\n",
    "                clean_tok = lemmatizer.lemmatize(tok)  # lemmatization\n",
    "                clean_tok = stemmer.stem(clean_tok)  # Stemming\n",
    "                clean_tokens.append(clean_tok)\n",
    "        return clean_tokens\n",
    "    else:\n",
    "        return []  # Return an empty list for NaN values\n",
    "\n",
    "# Read the dataset\n",
    "data = pd.read_csv('./mtsamples.csv')\n",
    "\n",
    "# Clean the text and tokenize\n",
    "data['tokenized_transcription'] = data['transcription'].apply(clean_text)\n",
    "\n",
    "# Train a Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=data['tokenized_transcription'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Check if the model was trained successfully\n",
    "if not word2vec_model.wv.key_to_index:\n",
    "    print(\"Word2Vec model not trained successfully. Check your data and parameters.\")\n",
    "else:\n",
    "    # Function to average Word2Vec vectors for a sentence\n",
    "    def average_word_vectors(words, model, vocabulary, num_features):\n",
    "        feature_vector = [model.wv[word] for word in words if word in vocabulary]\n",
    "        return np.mean(feature_vector, axis=0) if feature_vector else np.zeros(num_features)\n",
    "\n",
    "    # Function to calculate average Word2Vec vectors for a list of sentences\n",
    "    def get_average_word_vectors(sentences, model, vocabulary, num_features):\n",
    "        return np.array([average_word_vectors(sentence, model, vocabulary, num_features) for sentence in sentences])\n",
    "\n",
    "    # Transform the tokenized text to average Word2Vec vectors\n",
    "    X_w2v = get_average_word_vectors(data['tokenized_transcription'], word2vec_model, word2vec_model.wv.key_to_index, 100)\n",
    "\n",
    "    # Display the Word2Vec vectors\n",
    "    print(\"\\nWord2Vec Vectors:\")\n",
    "    print(X_w2v)\n",
    "\n",
    "# Print additional information about the Word2Vec model\n",
    "print(\"\\nWord2Vec Model Information:\")\n",
    "print(\"Vocabulary size:\", len(word2vec_model.wv))\n",
    "print(\"Training time (seconds):\", word2vec_model.total_train_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff43a4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming you already have Word2Vec vectors (X_w2v) and structured data (data) from previous steps\n",
    "\n",
    "# Step 1: Merge datasets based on a common identifier (e.g., sample_name)\n",
    "merged_data = pd.merge(data, pd.DataFrame(X_w2v), left_index=True, right_index=True)\n",
    "\n",
    "# Print the columns of the merged dataset\n",
    "print(\"Columns in the Merged Dataset:\")\n",
    "print(merged_data.columns)\n",
    "\n",
    "# Check if 'target_variable' is present in the merged dataset\n",
    "if 'target_variable' in merged_data.columns:\n",
    "    # Step 2: Separate features (X) and target variable (y)\n",
    "    X = merged_data.drop(['target_variable', 'transcription', 'keywords'], axis=1)  # Exclude non-numeric features\n",
    "    y = merged_data['target_variable']\n",
    "\n",
    "    # Encode categorical variables if needed (e.g., 'medical_specialty')\n",
    "    label_encoder = LabelEncoder()\n",
    "    X['medical_specialty'] = label_encoder.fit_transform(X['medical_specialty'])\n",
    "\n",
    "    # Step 3: Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Step 4: Train a model (e.g., RandomForestClassifier) using the combined features\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Step 5: Make predictions and evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "    # Display evaluation metrics\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Classification Report:\\n\", classification_rep)\n",
    "else:\n",
    "    print(\"'target_variable' not found in the merged dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76209ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\priya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\priya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          transcription  fall_risk\n",
      "0     SUBJECTIVE:,  This 23-year-old white female pr...          0\n",
      "1     PAST MEDICAL HISTORY:, He has difficulty climb...          0\n",
      "2     HISTORY OF PRESENT ILLNESS: , I have seen ABC ...          0\n",
      "3     2-D M-MODE: , ,1.  Left atrial enlargement wit...          0\n",
      "4     1.  The left ventricular cavity size and wall ...          0\n",
      "...                                                 ...        ...\n",
      "4994  HISTORY:,  I had the pleasure of meeting and e...          1\n",
      "4995  ADMITTING DIAGNOSIS: , Kawasaki disease.,DISCH...          0\n",
      "4996  SUBJECTIVE: , This is a 42-year-old white fema...          0\n",
      "4997  CHIEF COMPLAINT: , This 5-year-old male presen...          0\n",
      "4998  HISTORY: , A 34-year-old male presents today s...          0\n",
      "\n",
      "[4999 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to clean text and tokenize\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        english_stopwords = set(stopwords.words('english'))\n",
    "        text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "        tokens = word_tokenize(text)\n",
    "        stemmer = PorterStemmer()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        clean_tokens = [lemmatizer.lemmatize(stemmer.stem(tok)) for tok in tokens if tok.strip() not in english_stopwords]\n",
    "        return clean_tokens\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Read the dataset\n",
    "data = pd.read_csv('./mtsamples.csv')\n",
    "\n",
    "# Clean and tokenize the text\n",
    "data['tokenized_transcription'] = data['transcription'].apply(clean_text)\n",
    "\n",
    "# Fall-related keywords\n",
    "fall_keywords = [\n",
    "    'dizziness', 'balance issues', 'unsteady gait', 'mobility problems', 'history of falls',\n",
    "    'vertigo', 'trip', 'stumble', 'loss of consciousness', 'slip', 'instability', 'clumsiness',\n",
    "    'lightheadedness', 'faint', 'impaired coordination', 'weakness', 'fracture', 'injury',\n",
    "    'elderly fall risk', 'fall prevention', 'medication side effects', 'orthostatic hypotension',\n",
    "    'vision problems', 'neurological disorders', 'muscle weakness', 'altered mental status',\n",
    "    'environmental hazards', 'polypharmacy', 'foot problems', 'alcohol use', 'cognitive impairment',\n",
    "    'postural instability', 'sensory deficits', 'age-related changes', 'osteoporosis', 'fear of falling',\n",
    "    'poor proprioception', 'poor reflexes', 'frequent stumbling', 'poor depth perception',\n",
    "    'gait abnormalities', 'parkinsonism', 'muscle atrophy', 'syncope', 'hypotension', 'seizures',\n",
    "    'diabetic neuropathy', 'medication adjustments', 'poor coordination', 'fearful of falling',\n",
    "    'slow reaction time', 'abnormal posture', 'musculoskeletal problems', 'shuffling gait',\n",
    "    'impaired vision', 'fearful gait', 'neurological deficits', 'foot pain', 'foot deformities',\n",
    "    'urinary incontinence', 'impaired proprioception', 'lack of exercise', 'dehydration',\n",
    "    'inadequate lighting', 'improper footwear', 'cognitive decline', 'gastrointestinal issues',\n",
    "    'inadequate nutrition', 'joint pain', 'environmental obstacles', 'difficulty rising from a chair',\n",
    "    'difficulty with stairs', 'sedentary lifestyle', 'poor health status'\n",
    "]\n",
    "\n",
    "# Function to check if any fall-related keyword is present in the text\n",
    "def has_fall_keywords(tokens):\n",
    "    return any(keyword in tokens for keyword in fall_keywords)\n",
    "\n",
    "# Create a new column 'fall_risk' indicating the presence of fall-related keywords\n",
    "data['fall_risk'] = data['tokenized_transcription'].apply(has_fall_keywords).astype(int)\n",
    "\n",
    "# Display the dataset with the new 'fall_risk' column\n",
    "print(data[['transcription', 'fall_risk']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4716cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib  # Import joblib\n",
    "\n",
    "# Assuming you have a DataFrame 'data' with features and target variable\n",
    "# For illustration purposes, we'll create a simple DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'feature_1': [1, 2, 3, 4, 5],\n",
    "    'feature_2': [2, 3, 4, 5, 6],\n",
    "    'target': [0, 0, 1, 1, 0]\n",
    "})\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data[['feature_1', 'feature_2']]\n",
    "y = data['target']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a machine learning model (Random Forest for illustration)\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Save the trained model to a file using joblib\n",
    "joblib.dump(model, 'your_model_file.pkl')\n",
    "\n",
    "# Later, you can load the model using joblib\n",
    "loaded_model = joblib.load('your_model_file.pkl')\n",
    "\n",
    "# Now, you can use the loaded_model for making predictions on new data\n",
    "# For example:\n",
    "new_data = pd.DataFrame({'feature_1': [6], 'feature_2': [7]})\n",
    "prediction = loaded_model.predict(new_data)\n",
    "print(f\"Prediction for new data: {prediction}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
